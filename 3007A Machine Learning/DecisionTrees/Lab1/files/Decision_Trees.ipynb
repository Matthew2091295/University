{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMS3007 Decision Tree Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we will be using the data from the cards which we looked at last week. An example is shown below. The word and coloured box in the center of the card are the raw features of the data. The words in the corners of the cards are the labels for $4$ different classifications of the card. Each classification task has two classes, namely \"yes\" and \"no\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"files/data_example.png\"  width=\"300\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From last weeks lab we determined a set of features of the cards which we deemed to be helpful to determining how the cards are classified. These included, whether or not the word in the middle was a fruit or vegetable, if the word began in a vowel, or ended in a vowel, as well as the colour of the box. Thus, below you are given data with $5$ features (a $5$-tuple or $5$d vector), where index $0$ reflects if the word is a fruit (True or False), index $1$ reflects if the word starts with a vowel (True or False), index $2$ reflects if the word ends in a vowel (True or False), index $3$ reflects the colour of the box (indicated by numerical values from $0$ to $3$ with the range of colours being [blue,green,orange,red]) and lastly index $4$ reflects the word in the middle of the card (again represented by integer values from $0$ to $26$ with the list of words being [apple, apricot, asparagus, avocado, banana, bean, beet, blueberry, blackberry, broccoli, carrot, celery, \n",
    "cherry, cucumber, eggplant, fig, grape, lemon, lettuce, onion, orange, pea, pear, peach, plum, potato,spinach])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_problem = 0 # 0, 1, 2, 3 # Determines which classification problem is worked on 0=Top Left, 1=Top Right, 2=Bottom Left 3=Bottom Right\n",
    "feature_names = np.array(['is_fruit', 'starts_vowel', 'end_vowel', 'data_colours', 'data_words']) # List of feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_data has  1100  data points with  5 features\n",
      "full_y_values has labels for  1100  data points with  4 labels per data points\n"
     ]
    }
   ],
   "source": [
    "# Reading in the data points\n",
    "full_data, full_y_values = helper.read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use first 1000 data points from ``full_data'' as training data\n",
    "data = full_data[0:1000]\n",
    "y_values = full_y_values[0:1000]\n",
    "\n",
    "# TODO: Use last 100 data points from ``full_data'' as test data\n",
    "test_data = full_data[1000:]\n",
    "test_y_values = full_y_values[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data loaded and know what each feature means and its range of values we can start making the decision tree. Firstly we will write a function to calculate the entropy of a set of data. A reminder that the formula to calculate entropy is:\n",
    "$H(p) = - \\sum_{i=1}^n p_i \\log_2 p_i$ where $p_i$ is the probability of seeing a particular value for class $i$ out of any of the other possible value for the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4905765652494889"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO:\n",
    "# Write a function which calculates the entropy of a set of data (in general we will only pass catergory labels into the function)\n",
    "# HINT: look at np.unique, note unique returns in ascending order or alphabetically.\n",
    "# The function must return the entropy value and an array of the unique values in the data\n",
    "import math\n",
    "def calc_entropy(labels):\n",
    "    entrop = 0\n",
    "    for i, val in enumerate(np.unique(labels)):\n",
    "        count = np.count_nonzero(labels == val)\n",
    "        entrop = entrop + (-1*(count/len(labels))*math.log2(count/len(labels)))\n",
    "        return entrop\n",
    "calc_entropy(y_values[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now complete the class which will construct the decision tree below. The class itself can be found in the helper.py file but you don't need to look at it to complete the lab. We will just be completing two of the necessary functions to get it to work. Most of the code inside of helper.py is to help with drawing the tree later on, which you don't need to worry about. The primary step involved in constructing a decision tree is to find the feature which provides the most information gain (greatest decrease in entropy). Information Gain is calculated as: $Gain(D,F)=H(D) - \\frac{1}{|D|}\\sum_{f \\in values\\_of\\_F}|D_f| H(D_f)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# Complete the function which takes in the data and corresponding labels\n",
    "# at a node and picks the feature with the largest information gain.\n",
    "# The function must return the index of the chosen feature (so which column of the data gave the largest\n",
    "# information gain when we used it to split the data.)\n",
    "def pick_feature(self, data, labels):\n",
    "    for \n",
    "\n",
    "helper.tree_node.pick_feature = pick_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the function which picks a feature to split the data with, we must now add the next layer of the tree. This is what the ``descend_tree`` function is for below. Most of the function has been given to you. You just need to complete the line starting with ``data_for_feature_value``. This line needs to find the particular data points (rows) of ``data`` that have a certain value ``feature_value`` for the ``chosen_feature`` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def descend_tree(self, data, chosen_feature):\n",
    "        print(\"Descending tree\")\n",
    "        unique = np.unique(data[:,chosen_feature]) # Find the unique values of the chosen feature\n",
    "        for feature_value in unique: # For each unqiue value the chosen feature can take\n",
    "            \n",
    "            data_for_feature_value = # TODO: Find data where unique value for the feature occurs\n",
    "            \n",
    "            remaining_features = np.arange(data.shape[1])!=chosen_feature # Indicates which of the features haven't been used to split the data yet\n",
    "            helper.tree_node.add_child(self, data_for_feature_value, remaining_features) # Adds child node using the data with a particular value for the feature we split\n",
    "        self.node_values = unique # Update class values which holds the values of the feature it uses\n",
    "\n",
    "helper.tree_node.descend_tree = descend_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a class which we can use to make a decision tree we can now actually train a model on our data. In the below window we use every feature defined in our training data to construct the root of our decision tree. The class builds the tree recursively, so this will descend the entire tree. Now might be a good time to look at the class and make sure you understand the rest of the algorithm. In particular try understand the three cases in the ``__init__`` function as well as the ``infer`` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = helper.create_tree(class_problem, data, y_values, feature_names, calc_entropy, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next window is the same as the one above except that the decision tree here is trained without the last feature, the individual words category. This is to highlight the fact that decision trees tend to overfit the data. This can be seen with class_problem=2 or class_problem=3. In the cases where the data is separated by whether or not the word starts or ends in a vowel, then the data will also be separable by the individual words. This is undesirable, however, as the model has not found the general rule describing the data. The ability of decision trees to be very specific is a reason for their ability to overfit, as in noisy data the model will become specific about the noise. Note how much smaller the model becomes when we remove the feature which the model used to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root2 = helper.create_tree(class_problem, data[:,:-1], y_values, feature_names[:-1], calc_entropy, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we use our trained model above and run it on unseen data. This is the real test of how good of a model we have built!\n",
    "\n",
    "Since the first $3$ problems have no noise in them we would expect the models to be nearly perfect, even on unseen data (which also isn't noisy). For the last problem, however, 10% of the data labels in the training and test data have been flipped from their correct values. Thus, we can expect to see our model be incorrect roughly 10% of the time on this noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_correct = 0\n",
    "for i in range(data.shape[0]):\n",
    "    out = root.infer(data[i])\n",
    "    is_correct = (out == y_values[i,class_problem])\n",
    "    if is_correct:\n",
    "        count_correct = count_correct + 1\n",
    "print(\"Final Test Accuracy: \", count_correct/y_values.shape[0])\n",
    "# Note test accuracy should be around 90% cause I've flipped 10% of train data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_correct = 0\n",
    "for i in range(test_data.shape[0]):\n",
    "    out = root.infer(test_data[i])\n",
    "    is_correct = (out == test_y_values[i,class_problem])\n",
    "    if is_correct:\n",
    "        count_correct = count_correct + 1\n",
    "print(\"Final Test Accuracy: \", count_correct/test_y_values.shape[0])\n",
    "# Note test accuracy should be around 90% cause I've flipped 10% of train data labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
